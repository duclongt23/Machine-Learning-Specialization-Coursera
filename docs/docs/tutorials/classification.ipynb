{
  "cells": [
    {
      "cell_type": "raw",
      "id": "cb6f552e-775f-4d84-bc7c-dca94c06a33c",
      "metadata": {
        "id": "cb6f552e-775f-4d84-bc7c-dca94c06a33c"
      },
      "source": [
        "---\n",
        "title: Tagging\n",
        "sidebar_class_name: hidden\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0507a4b",
      "metadata": {
        "id": "a0507a4b"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/use_cases/tagging.ipynb)\n",
        "\n",
        "# Classify Text into Labels\n",
        "\n",
        "Tagging means labeling a document with classes such as:\n",
        "\n",
        "- Sentiment\n",
        "- Language\n",
        "- Style (formal, informal etc.)\n",
        "- Covered topics\n",
        "- Political tendency\n",
        "\n",
        "![Image description](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/tagging.png?raw=1)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Tagging has a few components:\n",
        "\n",
        "* `function`: Like [extraction](/docs/tutorials/extraction), tagging uses [functions](https://openai.com/blog/function-calling-and-other-api-updates) to specify how the model should tag a document\n",
        "* `schema`: defines how we want to tag the document\n",
        "\n",
        "## Quickstart\n",
        "\n",
        "Let's see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We'll use the [`with_structured_output`](/docs/how_to/structured_output) method supported by OpenAI models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dc5cbb6f",
      "metadata": {
        "id": "dc5cbb6f",
        "outputId": "f15487cd-a38a-44e2-f772-32c35b193a61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/417.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/417.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m409.6/417.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.1/417.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install --upgrade --quiet langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"LangSmith API key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key:\")"
      ],
      "metadata": {
        "id": "a0XgJz4cgUuF",
        "outputId": "a7603115-dd1c-42c9-c006-8a7f3f4c4abb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "a0XgJz4cgUuF",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API key:··········\n",
            "OpenAI API key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2b7cdf-babb-46e2-98d0-302f69446842",
      "metadata": {
        "id": "cc2b7cdf-babb-46e2-98d0-302f69446842"
      },
      "source": [
        "We'll need to load a [chat model](/docs/integrations/chat/):\n",
        "\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs customVarName=\"llm\" />"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain[openai]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XKtGrHA-g7VC",
        "outputId": "985aaa44-abb8-4f06-ca56-fc4eb8d73969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XKtGrHA-g7VC",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain[openai] in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (0.3.46)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (0.3.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (6.0.2)\n",
            "Collecting langchain-openai (from langchain[openai])\n",
            "  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain[openai]) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain[openai]) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain[openai]) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain[openai]) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[openai]) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[openai]) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[openai]) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[openai]) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[openai]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[openai]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[openai]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[openai]) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain[openai]) (3.1.1)\n",
            "Collecting openai<2.0.0,>=1.66.3 (from langchain-openai->langchain[openai])\n",
            "  Downloading openai-1.67.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai->langchain[openai])\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[openai]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[openai]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[openai]) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain[openai]) (3.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai->langchain[openai]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai->langchain[openai]) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai->langchain[openai]) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai->langchain[openai]) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai->langchain[openai]) (2024.11.6)\n",
            "Downloading langchain_openai-0.3.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.67.0-py3-none-any.whl (580 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m580.2/580.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.61.1\n",
            "    Uninstalling openai-1.61.1:\n",
            "      Successfully uninstalled openai-1.61.1\n",
            "Successfully installed langchain-openai-0.3.9 openai-1.67.0 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "608ee181-3f06-4719-842d-9672fdce6e57",
      "metadata": {
        "id": "608ee181-3f06-4719-842d-9672fdce6e57"
      },
      "outputs": [],
      "source": [
        "# | output: false\n",
        "# | echo: false\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ca3f93",
      "metadata": {
        "id": "b8ca3f93"
      },
      "source": [
        "Let's specify a Pydantic model with a few properties and their expected type in our schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "39f3ce3e",
      "metadata": {
        "id": "39f3ce3e"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "tagging_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Extract the desired information from the following passage.\n",
        "\n",
        "Only extract the properties mentioned in the 'Classification' function.\n",
        "\n",
        "Passage:\n",
        "{input}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "class Classification(BaseModel):\n",
        "  sentiment: str = Field(description='The sentiment of the text')\n",
        "  aggressiveness: int = Field(description='How aggressive the text is on a scale from 1 to 10')\n",
        "  language: str = Field(description='The language the text is written in')\n",
        "\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\").with_structured_output(\n",
        "    Classification\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5509b6a6",
      "metadata": {
        "id": "5509b6a6",
        "outputId": "ea0fbc6d-ea73-4b84-87b1-76dc87b21d6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classification(sentiment='positive', aggressiveness=1, language='Spanish')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff3cf30d",
      "metadata": {
        "id": "ff3cf30d"
      },
      "source": [
        "If we want dictionary output, we can just call `.model_dump()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9154474c",
      "metadata": {
        "id": "9154474c",
        "outputId": "ac2b7272-5672-4a46-f2e7-92c1aaf21e32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'angry', 'aggressiveness': 8, 'language': 'Spanish'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "response.model_dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d921bb53",
      "metadata": {
        "id": "d921bb53"
      },
      "source": [
        "As we can see in the examples, it correctly interprets what we want.\n",
        "\n",
        "The results vary so that we may get, for example, sentiments in different languages ('positive', 'enojado' etc.).\n",
        "\n",
        "We will see how to control these results in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebb2f83",
      "metadata": {
        "id": "bebb2f83"
      },
      "source": [
        "## Finer control\n",
        "\n",
        "Careful schema definition gives us more control over the model's output.\n",
        "\n",
        "Specifically, we can define:\n",
        "\n",
        "- Possible values for each property\n",
        "- Description to make sure that the model understands the property\n",
        "- Required properties to be returned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ef0b9a",
      "metadata": {
        "id": "69ef0b9a"
      },
      "source": [
        "Let's redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a5f7961",
      "metadata": {
        "id": "6a5f7961"
      },
      "outputs": [],
      "source": [
        "class Classification(BaseModel):\n",
        "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n",
        "    aggressiveness: int = Field(\n",
        "        ...,\n",
        "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
        "        enum=[1, 2, 3, 4, 5],\n",
        "    )\n",
        "    language: str = Field(\n",
        "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5a5881f",
      "metadata": {
        "id": "e5a5881f"
      },
      "outputs": [],
      "source": [
        "tagging_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Extract the desired information from the following passage.\n",
        "\n",
        "Only extract the properties mentioned in the 'Classification' function.\n",
        "\n",
        "Passage:\n",
        "{input}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\").with_structured_output(\n",
        "    Classification\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ded2332",
      "metadata": {
        "id": "5ded2332"
      },
      "source": [
        "Now the answers will be restricted in a way we expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b9d53d",
      "metadata": {
        "id": "d9b9d53d",
        "outputId": "f5f1ad01-0e71-4c8d-87e3-f56841cd6903"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Classification(sentiment='positive', aggressiveness=1, language='Spanish')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c12fa00",
      "metadata": {
        "id": "1c12fa00",
        "outputId": "593455a9-945a-4f5b-b91f-f95a5579f0c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Classification(sentiment='enojado', aggressiveness=8, language='es')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdfcb05",
      "metadata": {
        "id": "0bdfcb05",
        "outputId": "09b4e8c6-9b49-454d-8443-ee288ef8fdbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Classification(sentiment='neutral', aggressiveness=1, language='English')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp = \"Weather is ok here, I can go outside without much more than a coat\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6b7389",
      "metadata": {
        "id": "cf6b7389"
      },
      "source": [
        "The [LangSmith trace](https://smith.langchain.com/public/38294e04-33d8-4c5a-ae92-c2fe68be8332/r) lets us peek under the hood:\n",
        "\n",
        "![Image description](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/tagging_trace.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29346d09",
      "metadata": {
        "id": "29346d09"
      },
      "source": [
        "### Going deeper\n",
        "\n",
        "* You can use the [metadata tagger](/docs/integrations/document_transformers/openai_metadata_tagger) document transformer to extract metadata from a LangChain `Document`.\n",
        "* This covers the same basic functionality as the tagging chain, only applied to a LangChain `Document`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}